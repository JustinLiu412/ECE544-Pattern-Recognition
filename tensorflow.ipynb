{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(set_type):\n",
    "    \"\"\"Get data from files and storage them in a array. Return the data_set and label_set.\n",
    "    \n",
    "    set_type    the type of data set you want to build, including train dataset, dev dataset \n",
    "                and eval dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    data_path = {'train': 'train/lab/hw1train_labels.txt'} #find the label file path needed for building dataset\n",
    "    path_prefix = '/Users/changsongdong/Dropbox/UIUC/ECE544/hw/hw1/'\n",
    "    full_path = path_prefix + data_path[set_type] #find the full path of the label file\n",
    "    label_array = np.loadtxt(full_path, dtype='string') #load the label file into a array\n",
    "\n",
    "    #creat empty arrays to insert label and data\n",
    "    label_set = np.zeros(len(label_array))\n",
    "    data_set = np.zeros([len(label_array), 16])\n",
    "    #the first column of the label file is the label,\n",
    "    #the second column is the corresbonding data file nam\n",
    "    for i in range(len(label_array)): \n",
    "        #build the label set\n",
    "        label_set[i] = label_array[i][0] #insert label into label_set\n",
    "        #build the data set\n",
    "        with open(path_prefix + label_array[i][1]) as data_file:\n",
    "            data = data_file.readlines()[0].split() #find the data accoding to label\n",
    "        for j in range(len(data)):\n",
    "            data_set[i][j] = data[j] #insert data into the dataset\n",
    "            \n",
    "    data_set, label_set = nan_check(data_set, label_set) #delete the rows containing 'nan'\n",
    "\n",
    "    return data_set, label_set #return the data set and label set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nan_check(data, label):\n",
    "    \"\"\"Find out the rows in datasets and delete these rows\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    nan_rows = np.array(0); #define an array containg the no. of rows having 'nan'\n",
    "    \n",
    "    #collect all the numbers of 'nan'-data rows\n",
    "    for i in range(len(data)):\n",
    "        for j in range(16):\n",
    "            if str(data[i][j]) == 'nan':\n",
    "                nan_rows = np.append(nan_rows, i)\n",
    "    nan_rows = np.delete(nan_rows, 0) #delete the first element of nan_rows which was made to fit the append()\n",
    "    \n",
    "    return np.delete(data, nan_rows, 0), np.delete(label, nan_rows) #output the dataset whose 'nan'-data rows have been deleted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle(data_set, label_set):\n",
    "    \"\"\"Randomly shuffle the data and label\n",
    "    \n",
    "    data_set    the data samples\n",
    "    \n",
    "    label_set   the lables\n",
    "    \"\"\"\n",
    "    \n",
    "    shuffled_data = np.zeros((data_set.shape))\n",
    "    shuffled_label = np.zeros((len(label_set)))\n",
    "    idx = np.array(xrange(len(label_set)))\n",
    "    random.shuffle(idx)\n",
    "    i = 0\n",
    "    for j in idx:\n",
    "        shuffled_data[i] = data_set[int(j)]\n",
    "        shuffled_label[i] = label_set[int(j)]\n",
    "        i += 1\n",
    "    return shuffled_data, shuffled_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8774, 1)\n"
     ]
    }
   ],
   "source": [
    "train_data, train_label = get_data('train')\n",
    "train_data, train_label = shuffle(train_data, train_label)\n",
    "\n",
    "label = np.zeros((8774, 1))\n",
    "for i in range(len(train_label)):\n",
    "    label[i][0] = train_label[i]\n",
    "print label.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0: 5262.000000\n",
      "Loss at step 5: 5262.000000\n",
      "Loss at step 10: 5262.000000\n",
      "Loss at step 15: 5262.000000\n",
      "Loss at step 20: 5262.000000\n",
      "Loss at step 25: 5262.000000\n",
      "Loss at step 30: 5262.000000\n",
      "Loss at step 35: 5262.000000\n",
      "Loss at step 40: 5262.000000\n",
      "Loss at step 45: 5262.000000\n",
      "Loss at step 50: 5262.000000\n",
      "Loss at step 55: 5262.000000\n",
      "Loss at step 60: 5262.000000\n",
      "Loss at step 65: 5262.000000\n",
      "Loss at step 70: 5262.000000\n",
      "Loss at step 75: 5262.000000\n",
      "Loss at step 80: 5262.000000\n",
      "Loss at step 85: 5262.000000\n",
      "Loss at step 90: 5262.000000\n",
      "Loss at step 95: 5262.000000\n",
      "[[-0.52567858]\n",
      " [-0.52282906]\n",
      " [-0.44088125]\n",
      " [ 0.66628277]\n",
      " [-0.16787408]\n",
      " [ 0.19038929]\n",
      " [-1.37779593]\n",
      " [-0.42531368]\n",
      " [ 0.05143901]\n",
      " [ 0.31047881]\n",
      " [-0.43625134]\n",
      " [ 0.82192385]\n",
      " [ 0.44409898]\n",
      " [-1.17862022]\n",
      " [ 0.45010945]\n",
      " [-0.00670841]]\n"
     ]
    }
   ],
   "source": [
    "x_placeholder = tf.placeholder(tf.float32, [None, 16])\n",
    "y_placeholder = tf.placeholder(tf.float32, [None, 1])\n",
    "w = tf.Variable(tf.random_normal([16, 1]))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "y_hat = tf.matmul(x_placeholder, w) + b\n",
    "loss = tf.reduce_sum(tf.square(y_placeholder-y_hat))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.00000001).minimize(loss)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(100):\n",
    "    feed_dict = {x_placeholder: train_data, y_placeholder: label}\n",
    "    sess.run(train_step, feed_dict=feed_dict)\n",
    "    if step % 5 == 0:\n",
    "        print(\"Loss at step %d: %f\" % (step, loss_np))\n",
    "print sess.run(w)\n",
    "w_np = sess.run(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
