{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nan_check(data, label):\n",
    "    \"\"\"Find out the rows in datasets and delete these rows\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    nan_rows = np.array(0); #define an array containg the no. of rows having 'nan'\n",
    "    \n",
    "    #collect all the numbers of 'nan'-data rows\n",
    "    for i in range(len(data)):\n",
    "        for j in range(16):\n",
    "            if str(data[i][j]) == 'nan':\n",
    "                nan_rows = np.append(nan_rows, i)\n",
    "    nan_rows = np.delete(nan_rows, 0) #delete the first element of nan_rows which was made to fit the append()\n",
    "    \n",
    "    #output the dataset whose 'nan'-data rows have been deleted\n",
    "    return np.delete(data, nan_rows, 0), np.delete(label, nan_rows, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle(data_set, label_set):\n",
    "    \"\"\"Randomly shuffle the data and label\n",
    "    \n",
    "    data_set    the data samples\n",
    "    \n",
    "    label_set   the lables\n",
    "    \"\"\"\n",
    "    \n",
    "    shuffled_data = np.zeros((data_set.shape))\n",
    "    shuffled_label = np.zeros((label_set.shape))\n",
    "    idx = np.array(xrange(len(label_set)))\n",
    "    random.shuffle(idx)\n",
    "    i = 0\n",
    "    for j in idx:\n",
    "        shuffled_data[i] = data_set[int(j)]\n",
    "        shuffled_label[i] = label_set[int(j)]\n",
    "        i += 1\n",
    "    return shuffled_data, shuffled_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(set_type):\n",
    "    \"\"\"Get data from files and storage them in a array. Return the data_set and label_set.\n",
    "    \n",
    "    set_type    the type of data set you want to build, including train dataset, dev dataset \n",
    "                and eval dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    data_path = {'train': 'train/lab/hw1train_labels.txt', 'dev': 'dev/lab/hw1dev_labels.txt', \\\n",
    "                 'eval': 'eval/lab/hw1eval_labels.txt'} \n",
    "\n",
    "    label_array = np.loadtxt(data_path[set_type], dtype='string') #load the label file into a array\n",
    "\n",
    "    #creat empty arrays to insert label and data\n",
    "    label_set = np.zeros([len(label_array), 1])\n",
    "    data_set = np.zeros([len(label_array), 16])\n",
    "    \n",
    "    # the first column of the label file is the label,\n",
    "    # the second column is the corresbonding data file nam\n",
    "    for i in range(len(label_array)): \n",
    "        #build the label set\n",
    "        label_set[i] = label_array[i][0] # insert label into label_set\n",
    "        \n",
    "        #build the data set\n",
    "        with open(label_array[i][1]) as data_file:\n",
    "            data = data_file.readlines()[0].split() #find the data accoding to label\n",
    "        for j in range(len(data)):\n",
    "            data_set[i][j] = data[j] #insert data into the dataset\n",
    "            \n",
    "    data_set, label_set = nan_check(data_set, label_set) #delete the rows containing 'nan'\n",
    "\n",
    "    return shuffle(data_set, label_set) #return the shuffled data set and label set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_regression_gradient(data, label, weight, b):\n",
    "    \"\"\"Calculate the gradient of linear regression classifier. Return the gradient.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    gradient_w, gradient_b = 0, 0\n",
    "    for i in range(len(label)):\n",
    "        gradient_w += (-2) * (label[i] - (np.dot(weight, data[i]) + b)) * data[i]\n",
    "        gradient_b += (-2) * (label[i] - (np.dot(weight, data[i]) + b))\n",
    "\n",
    "    return gradient_w, gradient_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logistic_regression_gradient(data, label, weight, b):\n",
    "    \"\"\"Calculate the gradient of logistic regression . Return the gradient\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    gradient_w, gradient_b = 0, 0\n",
    "    for i in range(len(label)):\n",
    "        gradient_w += (-2) * ((np.dot(weight, data[i]) + b) - label[i]) * (np.dot(weight, data[i]) + b) * \\\n",
    "                   (1 - (np.dot(weight, data[i]) + b)) * data[i]\n",
    "        gradient_b += (-2) * ((np.dot(weight, data[i]) + b) - label[i]) * (np.dot(weight, data[i]) + b) * \\\n",
    "                   (1 - (np.dot(weight, data[i]) + b))\n",
    "        \n",
    "    return gradient_w / len(label), gradient_b / len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(weight, b, learning_rate, gradient_w, gradient_b):\n",
    "    \"\"\"Update and return weight and b.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    weight -= learning_rate * gradient_w\n",
    "    b -= learning_rate * gradient_b\n",
    "    return weight, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_MSE(data, label, weight, b, mse):\n",
    "    \"\"\"Compute the Mean Square Error\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(len(label)):\n",
    "        mse += (label[i] - (np.dot(weight, data[i]) + b)) ** 2\n",
    "        \n",
    "    mse = mse / len(label)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_mse(dev_data, dev_label, w, b):\n",
    "    \"\"\"Compute the mean square error\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    mse = 0\n",
    "    mse = compute_MSE(dev_data, dev_label, w, b, mse)\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_acc(data, label, w, b):\n",
    "    \"\"\"accuracy\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    acc = 0\n",
    "    for i in range(len(label)):\n",
    "        if label[i] == round(np.dot(w, data[i]) + b):\n",
    "            acc += 1\n",
    "    return acc / float(len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def activate(epoch = 1000, lr = 0.0001):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # data and parameter initialization\n",
    "    w = 2 * np.random.random(size = 16) - 1\n",
    "    b = 0\n",
    "\n",
    "    train_data, train_label = get_data('train') #build the dataset for training network\n",
    "    dev_data, dev_label = get_data('dev')\n",
    "    \n",
    "    for i in range(epoch):    \n",
    "        g_w, g_b = logistic_regression_gradient(train_data, train_label, w, b)\n",
    "        w, b = gradient_descent(w, b, lr, g_w, g_b)\n",
    "    \n",
    "        mse = compute_mse(dev_data, dev_label, w, b)\n",
    "        acc = compute_acc(dev_data, dev_label, w, b)\n",
    "        \n",
    "        print \"epoch %d, loss: %f, error rate: %s \" % (i, mse, 1 - acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 12.331356, error rate: 0.847286821705 \n",
      "epoch 1, loss: 11.834105, error rate: 0.844186046512 \n",
      "epoch 2, loss: 11.381098, error rate: 0.843410852713 \n",
      "epoch 3, loss: 10.966595, error rate: 0.838759689922 \n",
      "epoch 4, loss: 10.585818, error rate: 0.843410852713 \n",
      "epoch 5, loss: 10.234752, error rate: 0.841860465116 \n",
      "epoch 6, loss: 9.910003, error rate: 0.839534883721 \n",
      "epoch 7, loss: 9.608678, error rate: 0.838759689922 \n",
      "epoch 8, loss: 9.328298, error rate: 0.837209302326 \n",
      "epoch 9, loss: 9.066722, error rate: 0.835658914729 \n",
      "epoch 10, loss: 8.822097, error rate: 0.832558139535 \n",
      "epoch 11, loss: 8.592804, error rate: 0.829457364341 \n",
      "epoch 12, loss: 8.377428, error rate: 0.831007751938 \n",
      "epoch 13, loss: 8.174724, error rate: 0.831007751938 \n",
      "epoch 14, loss: 7.983590, error rate: 0.827131782946 \n",
      "epoch 15, loss: 7.803053, error rate: 0.827131782946 \n",
      "epoch 16, loss: 7.632243, error rate: 0.827131782946 \n",
      "epoch 17, loss: 7.470386, error rate: 0.826356589147 \n",
      "epoch 18, loss: 7.316788, error rate: 0.82480620155 \n",
      "epoch 19, loss: 7.170824, error rate: 0.825581395349 \n",
      "epoch 20, loss: 7.031933, error rate: 0.827906976744 \n",
      "epoch 21, loss: 6.899608, error rate: 0.827906976744 \n",
      "epoch 22, loss: 6.773387, error rate: 0.823255813953 \n",
      "epoch 23, loss: 6.652854, error rate: 0.825581395349 \n",
      "epoch 24, loss: 6.537627, error rate: 0.82480620155 \n",
      "epoch 25, loss: 6.427362, error rate: 0.82480620155 \n",
      "epoch 26, loss: 6.321739, error rate: 0.826356589147 \n",
      "epoch 27, loss: 6.220469, error rate: 0.827906976744 \n",
      "epoch 28, loss: 6.123286, error rate: 0.829457364341 \n",
      "epoch 29, loss: 6.029944, error rate: 0.828682170543 \n",
      "epoch 30, loss: 5.940218, error rate: 0.831007751938 \n",
      "epoch 31, loss: 5.853899, error rate: 0.831007751938 \n",
      "epoch 32, loss: 5.770795, error rate: 0.831007751938 \n",
      "epoch 33, loss: 5.690728, error rate: 0.827906976744 \n",
      "epoch 34, loss: 5.613533, error rate: 0.826356589147 \n",
      "epoch 35, loss: 5.539056, error rate: 0.82480620155 \n",
      "epoch 36, loss: 5.467155, error rate: 0.826356589147 \n",
      "epoch 37, loss: 5.397697, error rate: 0.827131782946 \n",
      "epoch 38, loss: 5.330559, error rate: 0.827906976744 \n",
      "epoch 39, loss: 5.265625, error rate: 0.827906976744 \n",
      "epoch 40, loss: 5.202788, error rate: 0.827906976744 \n",
      "epoch 41, loss: 5.141946, error rate: 0.827906976744 \n",
      "epoch 42, loss: 5.083005, error rate: 0.828682170543 \n",
      "epoch 43, loss: 5.025877, error rate: 0.831007751938 \n",
      "epoch 44, loss: 4.970479, error rate: 0.828682170543 \n",
      "epoch 45, loss: 4.916732, error rate: 0.828682170543 \n",
      "epoch 46, loss: 4.864563, error rate: 0.831007751938 \n",
      "epoch 47, loss: 4.813903, error rate: 0.831782945736 \n",
      "epoch 48, loss: 4.764688, error rate: 0.83023255814 \n",
      "epoch 49, loss: 4.716854, error rate: 0.828682170543 \n",
      "epoch 50, loss: 4.670346, error rate: 0.828682170543 \n",
      "epoch 51, loss: 4.625107, error rate: 0.828682170543 \n",
      "epoch 52, loss: 4.581087, error rate: 0.828682170543 \n",
      "epoch 53, loss: 4.538236, error rate: 0.828682170543 \n",
      "epoch 54, loss: 4.496508, error rate: 0.827906976744 \n",
      "epoch 55, loss: 4.455859, error rate: 0.827131782946 \n",
      "epoch 56, loss: 4.416248, error rate: 0.824031007752 \n",
      "epoch 57, loss: 4.377634, error rate: 0.823255813953 \n",
      "epoch 58, loss: 4.339981, error rate: 0.82480620155 \n",
      "epoch 59, loss: 4.303253, error rate: 0.825581395349 \n",
      "epoch 60, loss: 4.267416, error rate: 0.826356589147 \n",
      "epoch 61, loss: 4.232438, error rate: 0.827906976744 \n",
      "epoch 62, loss: 4.198288, error rate: 0.827906976744 \n",
      "epoch 63, loss: 4.164936, error rate: 0.829457364341 \n",
      "epoch 64, loss: 4.132355, error rate: 0.829457364341 \n",
      "epoch 65, loss: 4.100519, error rate: 0.827906976744 \n",
      "epoch 66, loss: 4.069402, error rate: 0.827906976744 \n",
      "epoch 67, loss: 4.038979, error rate: 0.82480620155 \n",
      "epoch 68, loss: 4.009228, error rate: 0.826356589147 \n",
      "epoch 69, loss: 3.980127, error rate: 0.824031007752 \n",
      "epoch 70, loss: 3.951654, error rate: 0.82480620155 \n",
      "epoch 71, loss: 3.923789, error rate: 0.824031007752 \n",
      "epoch 72, loss: 3.896513, error rate: 0.824031007752 \n",
      "epoch 73, loss: 3.869807, error rate: 0.822480620155 \n",
      "epoch 74, loss: 3.843653, error rate: 0.822480620155 \n",
      "epoch 75, loss: 3.818035, error rate: 0.819379844961 \n",
      "epoch 76, loss: 3.792936, error rate: 0.819379844961 \n",
      "epoch 77, loss: 3.768341, error rate: 0.818604651163 \n",
      "epoch 78, loss: 3.744233, error rate: 0.819379844961 \n",
      "epoch 79, loss: 3.720600, error rate: 0.818604651163 \n",
      "epoch 80, loss: 3.697427, error rate: 0.817829457364 \n",
      "epoch 81, loss: 3.674700, error rate: 0.816279069767 \n",
      "epoch 82, loss: 3.652407, error rate: 0.817829457364 \n",
      "epoch 83, loss: 3.630536, error rate: 0.818604651163 \n",
      "epoch 84, loss: 3.609074, error rate: 0.819379844961 \n",
      "epoch 85, loss: 3.588011, error rate: 0.817829457364 \n",
      "epoch 86, loss: 3.567334, error rate: 0.817054263566 \n",
      "epoch 87, loss: 3.547035, error rate: 0.817054263566 \n",
      "epoch 88, loss: 3.527101, error rate: 0.816279069767 \n",
      "epoch 89, loss: 3.507524, error rate: 0.815503875969 \n",
      "epoch 90, loss: 3.488295, error rate: 0.816279069767 \n",
      "epoch 91, loss: 3.469403, error rate: 0.814728682171 \n",
      "epoch 92, loss: 3.450840, error rate: 0.813953488372 \n",
      "epoch 93, loss: 3.432597, error rate: 0.813953488372 \n",
      "epoch 94, loss: 3.414667, error rate: 0.814728682171 \n",
      "epoch 95, loss: 3.397042, error rate: 0.813178294574 \n",
      "epoch 96, loss: 3.379713, error rate: 0.812403100775 \n",
      "epoch 97, loss: 3.362673, error rate: 0.812403100775 \n",
      "epoch 98, loss: 3.345915, error rate: 0.813178294574 \n",
      "epoch 99, loss: 3.329432, error rate: 0.813178294574 \n",
      "epoch 100, loss: 3.313218, error rate: 0.809302325581 \n",
      "epoch 101, loss: 3.297265, error rate: 0.807751937984 \n",
      "epoch 102, loss: 3.281568, error rate: 0.806201550388 \n",
      "epoch 103, loss: 3.266121, error rate: 0.805426356589 \n",
      "epoch 104, loss: 3.250917, error rate: 0.805426356589 \n",
      "epoch 105, loss: 3.235951, error rate: 0.806201550388 \n",
      "epoch 106, loss: 3.221218, error rate: 0.806201550388 \n",
      "epoch 107, loss: 3.206711, error rate: 0.806976744186 \n",
      "epoch 108, loss: 3.192427, error rate: 0.806201550388 \n",
      "epoch 109, loss: 3.178359, error rate: 0.806201550388 \n",
      "epoch 110, loss: 3.164503, error rate: 0.806976744186 \n",
      "epoch 111, loss: 3.150854, error rate: 0.806201550388 \n",
      "epoch 112, loss: 3.137408, error rate: 0.805426356589 \n",
      "epoch 113, loss: 3.124160, error rate: 0.805426356589 \n",
      "epoch 114, loss: 3.111105, error rate: 0.805426356589 \n",
      "epoch 115, loss: 3.098240, error rate: 0.804651162791 \n",
      "epoch 116, loss: 3.085561, error rate: 0.804651162791 \n",
      "epoch 117, loss: 3.073063, error rate: 0.804651162791 \n",
      "epoch 118, loss: 3.060743, error rate: 0.802325581395 \n",
      "epoch 119, loss: 3.048597, error rate: 0.802325581395 \n",
      "epoch 120, loss: 3.036621, error rate: 0.801550387597 \n",
      "epoch 121, loss: 3.024812, error rate: 0.801550387597 \n",
      "epoch 122, loss: 3.013166, error rate: 0.799224806202 \n",
      "epoch 123, loss: 3.001680, error rate: 0.799224806202 \n",
      "epoch 124, loss: 2.990351, error rate: 0.799224806202 \n",
      "epoch 125, loss: 2.979175, error rate: 0.799224806202 \n",
      "epoch 126, loss: 2.968150, error rate: 0.799224806202 \n",
      "epoch 127, loss: 2.957272, error rate: 0.801550387597 \n",
      "epoch 128, loss: 2.946539, error rate: 0.801550387597 \n",
      "epoch 129, loss: 2.935948, error rate: 0.798449612403 \n",
      "epoch 130, loss: 2.925495, error rate: 0.797674418605 \n",
      "epoch 131, loss: 2.915179, error rate: 0.797674418605 \n",
      "epoch 132, loss: 2.904996, error rate: 0.799224806202 \n",
      "epoch 133, loss: 2.894945, error rate: 0.798449612403 \n",
      "epoch 134, loss: 2.885021, error rate: 0.798449612403 \n",
      "epoch 135, loss: 2.875225, error rate: 0.799224806202 \n",
      "epoch 136, loss: 2.865551, error rate: 0.798449612403 \n",
      "epoch 137, loss: 2.856000, error rate: 0.796124031008 \n",
      "epoch 138, loss: 2.846567, error rate: 0.796899224806 \n",
      "epoch 139, loss: 2.837251, error rate: 0.797674418605 \n",
      "epoch 140, loss: 2.828051, error rate: 0.797674418605 \n",
      "epoch 141, loss: 2.818963, error rate: 0.797674418605 \n",
      "epoch 142, loss: 2.809986, error rate: 0.796124031008 \n",
      "epoch 143, loss: 2.801117, error rate: 0.796124031008 \n",
      "epoch 144, loss: 2.792356, error rate: 0.795348837209 \n",
      "epoch 145, loss: 2.783699, error rate: 0.795348837209 \n",
      "epoch 146, loss: 2.775145, error rate: 0.794573643411 \n",
      "epoch 147, loss: 2.766693, error rate: 0.793798449612 \n",
      "epoch 148, loss: 2.758340, error rate: 0.793023255814 \n",
      "epoch 149, loss: 2.750084, error rate: 0.792248062016 \n",
      "epoch 150, loss: 2.741925, error rate: 0.792248062016 \n",
      "epoch 151, loss: 2.733860, error rate: 0.792248062016 \n",
      "epoch 152, loss: 2.725888, error rate: 0.791472868217 \n",
      "epoch 153, loss: 2.718008, error rate: 0.792248062016 \n",
      "epoch 154, loss: 2.710217, error rate: 0.793023255814 \n",
      "epoch 155, loss: 2.702514, error rate: 0.791472868217 \n",
      "epoch 156, loss: 2.694898, error rate: 0.790697674419 \n",
      "epoch 157, loss: 2.687367, error rate: 0.78992248062 \n",
      "epoch 158, loss: 2.679920, error rate: 0.78992248062 \n",
      "epoch 159, loss: 2.672555, error rate: 0.789147286822 \n",
      "epoch 160, loss: 2.665272, error rate: 0.789147286822 \n",
      "epoch 161, loss: 2.658069, error rate: 0.789147286822 \n",
      "epoch 162, loss: 2.650944, error rate: 0.789147286822 \n",
      "epoch 163, loss: 2.643896, error rate: 0.789147286822 \n",
      "epoch 164, loss: 2.636925, error rate: 0.787596899225 \n",
      "epoch 165, loss: 2.630028, error rate: 0.789147286822 \n",
      "epoch 166, loss: 2.623205, error rate: 0.787596899225 \n",
      "epoch 167, loss: 2.616455, error rate: 0.786821705426 \n",
      "epoch 168, loss: 2.609776, error rate: 0.786821705426 \n",
      "epoch 169, loss: 2.603167, error rate: 0.786821705426 \n",
      "epoch 170, loss: 2.596628, error rate: 0.786821705426 \n",
      "epoch 171, loss: 2.590156, error rate: 0.786821705426 \n",
      "epoch 172, loss: 2.583752, error rate: 0.786046511628 \n",
      "epoch 173, loss: 2.577414, error rate: 0.785271317829 \n",
      "epoch 174, loss: 2.571141, error rate: 0.786046511628 \n",
      "epoch 175, loss: 2.564932, error rate: 0.785271317829 \n",
      "epoch 176, loss: 2.558786, error rate: 0.785271317829 \n",
      "epoch 177, loss: 2.552702, error rate: 0.785271317829 \n",
      "epoch 178, loss: 2.546680, error rate: 0.785271317829 \n",
      "epoch 179, loss: 2.540718, error rate: 0.782945736434 \n",
      "epoch 180, loss: 2.534815, error rate: 0.782945736434 \n",
      "epoch 181, loss: 2.528971, error rate: 0.782170542636 \n",
      "epoch 182, loss: 2.523185, error rate: 0.782170542636 \n",
      "epoch 183, loss: 2.517456, error rate: 0.781395348837 \n",
      "epoch 184, loss: 2.511783, error rate: 0.77984496124 \n",
      "epoch 185, loss: 2.506165, error rate: 0.77984496124 \n",
      "epoch 186, loss: 2.500602, error rate: 0.77984496124 \n",
      "epoch 187, loss: 2.495092, error rate: 0.779069767442 \n",
      "epoch 188, loss: 2.489636, error rate: 0.779069767442 \n",
      "epoch 189, loss: 2.484231, error rate: 0.77984496124 \n",
      "epoch 190, loss: 2.478878, error rate: 0.77984496124 \n",
      "epoch 191, loss: 2.473576, error rate: 0.77984496124 \n",
      "epoch 192, loss: 2.468325, error rate: 0.778294573643 \n",
      "epoch 193, loss: 2.463122, error rate: 0.776744186047 \n",
      "epoch 194, loss: 2.457968, error rate: 0.775968992248 \n",
      "epoch 195, loss: 2.452863, error rate: 0.77519379845 \n",
      "epoch 196, loss: 2.447804, error rate: 0.77519379845 \n",
      "epoch 197, loss: 2.442793, error rate: 0.774418604651 \n",
      "epoch 198, loss: 2.437827, error rate: 0.773643410853 \n",
      "epoch 199, loss: 2.432907, error rate: 0.773643410853 \n",
      "epoch 200, loss: 2.428032, error rate: 0.774418604651 \n",
      "epoch 201, loss: 2.423202, error rate: 0.774418604651 \n",
      "epoch 202, loss: 2.418415, error rate: 0.773643410853 \n",
      "epoch 203, loss: 2.413671, error rate: 0.773643410853 \n",
      "epoch 204, loss: 2.408970, error rate: 0.773643410853 \n",
      "epoch 205, loss: 2.404311, error rate: 0.772868217054 \n",
      "epoch 206, loss: 2.399693, error rate: 0.772093023256 \n",
      "epoch 207, loss: 2.395116, error rate: 0.772093023256 \n",
      "epoch 208, loss: 2.390580, error rate: 0.772093023256 \n",
      "epoch 209, loss: 2.386083, error rate: 0.771317829457 \n",
      "epoch 210, loss: 2.381626, error rate: 0.771317829457 \n",
      "epoch 211, loss: 2.377208, error rate: 0.771317829457 \n",
      "epoch 212, loss: 2.372828, error rate: 0.770542635659 \n",
      "epoch 213, loss: 2.368486, error rate: 0.770542635659 \n",
      "epoch 214, loss: 2.364181, error rate: 0.76976744186 \n",
      "epoch 215, loss: 2.359914, error rate: 0.770542635659 \n",
      "epoch 216, loss: 2.355682, error rate: 0.76976744186 \n",
      "epoch 217, loss: 2.351487, error rate: 0.768992248062 \n",
      "epoch 218, loss: 2.347328, error rate: 0.768992248062 \n",
      "epoch 219, loss: 2.343204, error rate: 0.76976744186 \n",
      "epoch 220, loss: 2.339114, error rate: 0.76976744186 \n",
      "epoch 221, loss: 2.335059, error rate: 0.76976744186 \n",
      "epoch 222, loss: 2.331037, error rate: 0.76976744186 \n",
      "epoch 223, loss: 2.327049, error rate: 0.76976744186 \n",
      "epoch 224, loss: 2.323094, error rate: 0.76976744186 \n",
      "epoch 225, loss: 2.319172, error rate: 0.768992248062 \n",
      "epoch 226, loss: 2.315282, error rate: 0.768992248062 \n",
      "epoch 227, loss: 2.311424, error rate: 0.768992248062 \n",
      "epoch 228, loss: 2.307598, error rate: 0.768992248062 \n",
      "epoch 229, loss: 2.303802, error rate: 0.768217054264 \n",
      "epoch 230, loss: 2.300038, error rate: 0.768217054264 \n",
      "epoch 231, loss: 2.296303, error rate: 0.768992248062 \n",
      "epoch 232, loss: 2.292599, error rate: 0.768992248062 \n",
      "epoch 233, loss: 2.288925, error rate: 0.768217054264 \n",
      "epoch 234, loss: 2.285280, error rate: 0.768992248062 \n",
      "epoch 235, loss: 2.281664, error rate: 0.768992248062 \n",
      "epoch 236, loss: 2.278076, error rate: 0.768992248062 \n",
      "epoch 237, loss: 2.274517, error rate: 0.76976744186 \n",
      "epoch 238, loss: 2.270986, error rate: 0.76976744186 \n",
      "epoch 239, loss: 2.267483, error rate: 0.768992248062 \n",
      "epoch 240, loss: 2.264007, error rate: 0.768217054264 \n",
      "epoch 241, loss: 2.260558, error rate: 0.768217054264 \n",
      "epoch 242, loss: 2.257136, error rate: 0.767441860465 \n",
      "epoch 243, loss: 2.253741, error rate: 0.767441860465 \n",
      "epoch 244, loss: 2.250371, error rate: 0.767441860465 \n",
      "epoch 245, loss: 2.247028, error rate: 0.767441860465 \n",
      "epoch 246, loss: 2.243710, error rate: 0.767441860465 \n",
      "epoch 247, loss: 2.240417, error rate: 0.768217054264 \n",
      "epoch 248, loss: 2.237150, error rate: 0.768217054264 \n",
      "epoch 249, loss: 2.233907, error rate: 0.767441860465 \n",
      "epoch 250, loss: 2.230689, error rate: 0.767441860465 \n",
      "epoch 251, loss: 2.227495, error rate: 0.767441860465 \n",
      "epoch 252, loss: 2.224325, error rate: 0.765891472868 \n",
      "epoch 253, loss: 2.221178, error rate: 0.765891472868 \n",
      "epoch 254, loss: 2.218056, error rate: 0.765891472868 \n",
      "epoch 255, loss: 2.214956, error rate: 0.765891472868 \n",
      "epoch 256, loss: 2.211879, error rate: 0.766666666667 \n",
      "epoch 257, loss: 2.208825, error rate: 0.765891472868 \n",
      "epoch 258, loss: 2.205793, error rate: 0.76511627907 \n",
      "epoch 259, loss: 2.202784, error rate: 0.762790697674 \n",
      "epoch 260, loss: 2.199797, error rate: 0.762790697674 \n",
      "epoch 261, loss: 2.196831, error rate: 0.762790697674 \n",
      "epoch 262, loss: 2.193887, error rate: 0.762015503876 \n",
      "epoch 263, loss: 2.190964, error rate: 0.762015503876 \n",
      "epoch 264, loss: 2.188063, error rate: 0.762015503876 \n",
      "epoch 265, loss: 2.185182, error rate: 0.762015503876 \n",
      "epoch 266, loss: 2.182321, error rate: 0.762015503876 \n",
      "epoch 267, loss: 2.179482, error rate: 0.761240310078 \n",
      "epoch 268, loss: 2.176662, error rate: 0.760465116279 \n",
      "epoch 269, loss: 2.173863, error rate: 0.760465116279 \n",
      "epoch 270, loss: 2.171083, error rate: 0.760465116279 \n",
      "epoch 271, loss: 2.168323, error rate: 0.759689922481 \n",
      "epoch 272, loss: 2.165582, error rate: 0.759689922481 \n",
      "epoch 273, loss: 2.162861, error rate: 0.762015503876 \n",
      "epoch 274, loss: 2.160158, error rate: 0.762790697674 \n",
      "epoch 275, loss: 2.157474, error rate: 0.764341085271 \n",
      "epoch 276, loss: 2.154809, error rate: 0.763565891473 \n",
      "epoch 277, loss: 2.152163, error rate: 0.762790697674 \n",
      "epoch 278, loss: 2.149534, error rate: 0.763565891473 \n",
      "epoch 279, loss: 2.146924, error rate: 0.764341085271 \n",
      "epoch 280, loss: 2.144332, error rate: 0.764341085271 \n",
      "epoch 281, loss: 2.141757, error rate: 0.764341085271 \n",
      "epoch 282, loss: 2.139200, error rate: 0.764341085271 \n",
      "epoch 283, loss: 2.136661, error rate: 0.764341085271 \n",
      "epoch 284, loss: 2.134138, error rate: 0.76511627907 \n",
      "epoch 285, loss: 2.131633, error rate: 0.76511627907 \n",
      "epoch 286, loss: 2.129144, error rate: 0.764341085271 \n",
      "epoch 287, loss: 2.126672, error rate: 0.762790697674 \n",
      "epoch 288, loss: 2.124217, error rate: 0.762790697674 \n",
      "epoch 289, loss: 2.121778, error rate: 0.762790697674 \n",
      "epoch 290, loss: 2.119356, error rate: 0.762790697674 \n",
      "epoch 291, loss: 2.116949, error rate: 0.762790697674 \n",
      "epoch 292, loss: 2.114558, error rate: 0.762790697674 \n",
      "epoch 293, loss: 2.112184, error rate: 0.762015503876 \n",
      "epoch 294, loss: 2.109824, error rate: 0.762015503876 \n",
      "epoch 295, loss: 2.107481, error rate: 0.761240310078 \n",
      "epoch 296, loss: 2.105152, error rate: 0.760465116279 \n",
      "epoch 297, loss: 2.102839, error rate: 0.760465116279 \n",
      "epoch 298, loss: 2.100541, error rate: 0.760465116279 \n",
      "epoch 299, loss: 2.098257, error rate: 0.760465116279 \n",
      "epoch 300, loss: 2.095989, error rate: 0.760465116279 \n",
      "epoch 301, loss: 2.093735, error rate: 0.760465116279 \n",
      "epoch 302, loss: 2.091496, error rate: 0.759689922481 \n",
      "epoch 303, loss: 2.089271, error rate: 0.758914728682 \n",
      "epoch 304, loss: 2.087060, error rate: 0.758914728682 \n",
      "epoch 305, loss: 2.084863, error rate: 0.758914728682 \n",
      "epoch 306, loss: 2.082680, error rate: 0.758139534884 \n",
      "epoch 307, loss: 2.080511, error rate: 0.757364341085 \n",
      "epoch 308, loss: 2.078356, error rate: 0.757364341085 \n",
      "epoch 309, loss: 2.076214, error rate: 0.758139534884 \n",
      "epoch 310, loss: 2.074086, error rate: 0.758139534884 \n",
      "epoch 311, loss: 2.071971, error rate: 0.758139534884 \n",
      "epoch 312, loss: 2.069870, error rate: 0.758139534884 \n",
      "epoch 313, loss: 2.067781, error rate: 0.757364341085 \n",
      "epoch 314, loss: 2.065706, error rate: 0.757364341085 \n",
      "epoch 315, loss: 2.063643, error rate: 0.757364341085 \n",
      "epoch 316, loss: 2.061593, error rate: 0.757364341085 \n",
      "epoch 317, loss: 2.059556, error rate: 0.758139534884 \n",
      "epoch 318, loss: 2.057531, error rate: 0.757364341085 \n",
      "epoch 319, loss: 2.055519, error rate: 0.757364341085 \n",
      "epoch 320, loss: 2.053519, error rate: 0.757364341085 \n",
      "epoch 321, loss: 2.051531, error rate: 0.756589147287 \n",
      "epoch 322, loss: 2.049555, error rate: 0.756589147287 \n",
      "epoch 323, loss: 2.047592, error rate: 0.75503875969 \n",
      "epoch 324, loss: 2.045640, error rate: 0.754263565891 \n",
      "epoch 325, loss: 2.043700, error rate: 0.754263565891 \n",
      "epoch 326, loss: 2.041771, error rate: 0.752713178295 \n",
      "epoch 327, loss: 2.039855, error rate: 0.751937984496 \n",
      "epoch 328, loss: 2.037949, error rate: 0.752713178295 \n",
      "epoch 329, loss: 2.036055, error rate: 0.752713178295 \n",
      "epoch 330, loss: 2.034173, error rate: 0.753488372093 \n",
      "epoch 331, loss: 2.032301, error rate: 0.753488372093 \n",
      "epoch 332, loss: 2.030441, error rate: 0.753488372093 \n",
      "epoch 333, loss: 2.028591, error rate: 0.752713178295 \n",
      "epoch 334, loss: 2.026753, error rate: 0.752713178295 \n",
      "epoch 335, loss: 2.024925, error rate: 0.753488372093 \n",
      "epoch 336, loss: 2.023108, error rate: 0.752713178295 \n",
      "epoch 337, loss: 2.021302, error rate: 0.751937984496 \n",
      "epoch 338, loss: 2.019506, error rate: 0.751937984496 \n",
      "epoch 339, loss: 2.017720, error rate: 0.751937984496 \n",
      "epoch 340, loss: 2.015945, error rate: 0.751937984496 \n",
      "epoch 341, loss: 2.014180, error rate: 0.751937984496 \n",
      "epoch 342, loss: 2.012426, error rate: 0.751937984496 \n",
      "epoch 343, loss: 2.010681, error rate: 0.751937984496 \n",
      "epoch 344, loss: 2.008947, error rate: 0.751937984496 \n",
      "epoch 345, loss: 2.007222, error rate: 0.751937984496 \n",
      "epoch 346, loss: 2.005508, error rate: 0.751937984496 \n",
      "epoch 347, loss: 2.003803, error rate: 0.751937984496 \n",
      "epoch 348, loss: 2.002108, error rate: 0.751937984496 \n",
      "epoch 349, loss: 2.000422, error rate: 0.751162790698 \n",
      "epoch 350, loss: 1.998746, error rate: 0.751162790698 \n",
      "epoch 351, loss: 1.997079, error rate: 0.750387596899 \n",
      "epoch 352, loss: 1.995422, error rate: 0.751162790698 \n",
      "epoch 353, loss: 1.993774, error rate: 0.751162790698 \n",
      "epoch 354, loss: 1.992135, error rate: 0.751162790698 \n",
      "epoch 355, loss: 1.990506, error rate: 0.751162790698 \n",
      "epoch 356, loss: 1.988885, error rate: 0.751937984496 \n",
      "epoch 357, loss: 1.987274, error rate: 0.751937984496 \n",
      "epoch 358, loss: 1.985671, error rate: 0.751937984496 \n",
      "epoch 359, loss: 1.984078, error rate: 0.751162790698 \n",
      "epoch 360, loss: 1.982493, error rate: 0.751162790698 \n",
      "epoch 361, loss: 1.980916, error rate: 0.751162790698 \n",
      "epoch 362, loss: 1.979349, error rate: 0.751937984496 \n",
      "epoch 363, loss: 1.977790, error rate: 0.751937984496 \n",
      "epoch 364, loss: 1.976240, error rate: 0.751937984496 \n",
      "epoch 365, loss: 1.974698, error rate: 0.751162790698 \n",
      "epoch 366, loss: 1.973164, error rate: 0.751162790698 \n",
      "epoch 367, loss: 1.971639, error rate: 0.751937984496 \n",
      "epoch 368, loss: 1.970122, error rate: 0.752713178295 \n",
      "epoch 369, loss: 1.968613, error rate: 0.752713178295 \n",
      "epoch 370, loss: 1.967112, error rate: 0.752713178295 \n",
      "epoch 371, loss: 1.965619, error rate: 0.753488372093 \n",
      "epoch 372, loss: 1.964135, error rate: 0.753488372093 \n",
      "epoch 373, loss: 1.962658, error rate: 0.753488372093 \n",
      "epoch 374, loss: 1.961189, error rate: 0.754263565891 \n",
      "epoch 375, loss: 1.959728, error rate: 0.754263565891 \n",
      "epoch 376, loss: 1.958275, error rate: 0.75503875969 \n",
      "epoch 377, loss: 1.956829, error rate: 0.754263565891 \n",
      "epoch 378, loss: 1.955391, error rate: 0.755813953488 \n",
      "epoch 379, loss: 1.953961, error rate: 0.755813953488 \n",
      "epoch 380, loss: 1.952538, error rate: 0.75503875969 \n",
      "epoch 381, loss: 1.951122, error rate: 0.753488372093 \n",
      "epoch 382, loss: 1.949714, error rate: 0.753488372093 \n",
      "epoch 383, loss: 1.948314, error rate: 0.753488372093 \n",
      "epoch 384, loss: 1.946920, error rate: 0.753488372093 \n",
      "epoch 385, loss: 1.945534, error rate: 0.753488372093 \n",
      "epoch 386, loss: 1.944155, error rate: 0.754263565891 \n",
      "epoch 387, loss: 1.942783, error rate: 0.754263565891 \n",
      "epoch 388, loss: 1.941419, error rate: 0.754263565891 \n",
      "epoch 389, loss: 1.940061, error rate: 0.753488372093 \n",
      "epoch 390, loss: 1.938710, error rate: 0.751937984496 \n",
      "epoch 391, loss: 1.937366, error rate: 0.752713178295 \n",
      "epoch 392, loss: 1.936029, error rate: 0.752713178295 \n",
      "epoch 393, loss: 1.934699, error rate: 0.752713178295 \n",
      "epoch 394, loss: 1.933375, error rate: 0.751937984496 \n",
      "epoch 395, loss: 1.932059, error rate: 0.752713178295 \n",
      "epoch 396, loss: 1.930749, error rate: 0.752713178295 \n",
      "epoch 397, loss: 1.929445, error rate: 0.751162790698 \n",
      "epoch 398, loss: 1.928148, error rate: 0.751937984496 \n",
      "epoch 399, loss: 1.926858, error rate: 0.751162790698 \n",
      "epoch 400, loss: 1.925574, error rate: 0.751162790698 \n",
      "epoch 401, loss: 1.924297, error rate: 0.751162790698 \n",
      "epoch 402, loss: 1.923026, error rate: 0.751162790698 \n",
      "epoch 403, loss: 1.921761, error rate: 0.751162790698 \n",
      "epoch 404, loss: 1.920502, error rate: 0.751162790698 \n",
      "epoch 405, loss: 1.919250, error rate: 0.751937984496 \n",
      "epoch 406, loss: 1.918004, error rate: 0.751162790698 \n",
      "epoch 407, loss: 1.916764, error rate: 0.751937984496 \n",
      "epoch 408, loss: 1.915530, error rate: 0.751937984496 \n",
      "epoch 409, loss: 1.914302, error rate: 0.751937984496 \n",
      "epoch 410, loss: 1.913081, error rate: 0.752713178295 \n",
      "epoch 411, loss: 1.911865, error rate: 0.753488372093 \n",
      "epoch 412, loss: 1.910655, error rate: 0.753488372093 \n",
      "epoch 413, loss: 1.909451, error rate: 0.752713178295 \n",
      "epoch 414, loss: 1.908253, error rate: 0.752713178295 \n",
      "epoch 415, loss: 1.907061, error rate: 0.751937984496 \n",
      "epoch 416, loss: 1.905874, error rate: 0.752713178295 \n",
      "epoch 417, loss: 1.904693, error rate: 0.752713178295 \n",
      "epoch 418, loss: 1.903518, error rate: 0.752713178295 \n",
      "epoch 419, loss: 1.902349, error rate: 0.752713178295 \n",
      "epoch 420, loss: 1.901185, error rate: 0.753488372093 \n"
     ]
    }
   ],
   "source": [
    "activate(2000, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
