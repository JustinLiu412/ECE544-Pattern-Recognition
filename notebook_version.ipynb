{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nan_check(data, label):\n",
    "    \"\"\"Find out the rows in datasets and delete these rows\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    nan_rows = np.array(0); #define an array containg the no. of rows having 'nan'\n",
    "    \n",
    "    #collect all the numbers of 'nan'-data rows\n",
    "    for i in range(len(data)):\n",
    "        for j in range(16):\n",
    "            if str(data[i][j]) == 'nan':\n",
    "                nan_rows = np.append(nan_rows, i)\n",
    "    nan_rows = np.delete(nan_rows, 0) #delete the first element of nan_rows which was made to fit the append()\n",
    "    \n",
    "    #output the dataset whose 'nan'-data rows have been deleted\n",
    "    return np.delete(data, nan_rows, 0), np.delete(label, nan_rows, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle(data_set, label_set):\n",
    "    \"\"\"Randomly shuffle the data and label\n",
    "    \n",
    "    data_set    the data samples\n",
    "    \n",
    "    label_set   the lables\n",
    "    \"\"\"\n",
    "    \n",
    "    shuffled_data = np.zeros((data_set.shape))\n",
    "    shuffled_label = np.zeros((label_set.shape))\n",
    "    idx = np.array(xrange(len(label_set)))\n",
    "    random.shuffle(idx)\n",
    "    i = 0\n",
    "    for j in idx:\n",
    "        shuffled_data[i] = data_set[int(j)]\n",
    "        shuffled_label[i] = label_set[int(j)]\n",
    "        i += 1\n",
    "    return shuffled_data, shuffled_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(set_type):\n",
    "    \"\"\"Get data from files and storage them in a array. Return the data_set and label_set.\n",
    "    \n",
    "    set_type    the type of data set you want to build, including train dataset, dev dataset \n",
    "                and eval dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    data_path = {'train': 'train/lab/hw1train_labels.txt', 'dev': 'dev/lab/hw1dev_labels.txt', \\\n",
    "                 'eval': 'eval/lab/hw1eval_labels.txt'} \n",
    "\n",
    "    label_array = np.loadtxt(data_path[set_type], dtype='string') #load the label file into a array\n",
    "\n",
    "    #creat empty arrays to insert label and data\n",
    "    label_set = np.zeros([len(label_array), 1])\n",
    "    data_set = np.zeros([len(label_array), 16])\n",
    "    \n",
    "    # the first column of the label file is the label,\n",
    "    # the second column is the corresbonding data file nam\n",
    "    for i in range(len(label_array)): \n",
    "        #build the label set\n",
    "        label_set[i] = label_array[i][0] # insert label into label_set\n",
    "        \n",
    "        #build the data set\n",
    "        with open(label_array[i][1]) as data_file:\n",
    "            data = data_file.readlines()[0].split() #find the data accoding to label\n",
    "        for j in range(len(data)):\n",
    "            data_set[i][j] = data[j] #insert data into the dataset\n",
    "            \n",
    "    data_set, label_set = nan_check(data_set, label_set) #delete the rows containing 'nan'\n",
    "\n",
    "    return shuffle(data_set, label_set) #return the shuffled data set and label set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_regression_gradient(data, label, weight, b):\n",
    "    \"\"\"Calculate the gradient of linear node classifier. Return the gradient.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    gradient_w, gradient_b = 0, 0\n",
    "    for i in range(len(label)):\n",
    "        gradient_w += (-2) * (label[i] - (np.dot(weight, data[i]) + b)) * data[i]\n",
    "        gradient_b += (-2) * (label[i] - (np.dot(weight, data[i]) + b))\n",
    "\n",
    "    return gradient_w, gradient_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(weight, b, learning_rate, gradient_w, gradient_b):\n",
    "    \"\"\"Update and return weight and b.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    weight -= learning_rate * gradient_w\n",
    "    b -= learning_rate * gradient_b\n",
    "    return weight, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_MSE(data, label, weight, b, mse):\n",
    "    \"\"\"Compute the Mean Square Error\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(len(label)):\n",
    "        mse += (label[i] - (np.dot(weight, data[i]) + b)) ** 2\n",
    "        \n",
    "    mse = mse / len(label)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_mse(dev_data, dev_label, w, b):\n",
    "    \"\"\"Compute the mean square error\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    mse = 0\n",
    "    mse = compute_MSE(dev_data, dev_label, w, b, mse)\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_acc(data, label, w, b):\n",
    "    \"\"\"accuracy\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    acc = 0\n",
    "    for i in range(len(label)):\n",
    "        if label[i] == round(np.dot(w, data[i]) + b):\n",
    "            acc += 1\n",
    "    return acc / float(len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def activate(epoch = 300, lr = 0.000001):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # data and parameter initialization\n",
    "    w = 2 * np.random.random(size = 16) - 1\n",
    "    b = 0\n",
    "\n",
    "    train_data, train_label = get_data('train') #build the dataset for training network\n",
    "    dev_data, dev_label = get_data('dev')\n",
    "    \n",
    "    for i in range(epoch):    \n",
    "        g_w, g_b = linear_regression_gradient(train_data, train_label, w, b)\n",
    "        w, b = gradient_descent(w, b, lr, g_w, g_b)\n",
    "    \n",
    "        mse = compute_mse(dev_data, dev_label, w, b)\n",
    "        acc = compute_acc(dev_data, dev_label, w, b)\n",
    "        \n",
    "        print(\"epoch %d, loss: %f, error rate: %s \" % (i, mse, 1 - acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 5.423835, error rate: 0.820930232558 \n",
      "epoch 1, loss: 2.971491, error rate: 0.749612403101 \n",
      "epoch 2, loss: 2.669611, error rate: 0.73488372093 \n",
      "epoch 3, loss: 2.541482, error rate: 0.728682170543 \n",
      "epoch 4, loss: 2.441614, error rate: 0.724031007752 \n",
      "epoch 5, loss: 2.352348, error rate: 0.723255813953 \n",
      "epoch 6, loss: 2.270014, error rate: 0.713178294574 \n",
      "epoch 7, loss: 2.193340, error rate: 0.709302325581 \n",
      "epoch 8, loss: 2.121609, error rate: 0.704651162791 \n",
      "epoch 9, loss: 2.054287, error rate: 0.698449612403 \n",
      "epoch 10, loss: 1.990933, error rate: 0.696899224806 \n",
      "epoch 11, loss: 1.931170, error rate: 0.692248062016 \n",
      "epoch 12, loss: 1.874676, error rate: 0.686821705426 \n",
      "epoch 13, loss: 1.821167, error rate: 0.681395348837 \n",
      "epoch 14, loss: 1.770400, error rate: 0.680620155039 \n",
      "epoch 15, loss: 1.722157, error rate: 0.67519379845 \n",
      "epoch 16, loss: 1.676249, error rate: 0.672868217054 \n",
      "epoch 17, loss: 1.632506, error rate: 0.667441860465 \n",
      "epoch 18, loss: 1.590776, error rate: 0.664341085271 \n",
      "epoch 19, loss: 1.550926, error rate: 0.658914728682 \n",
      "epoch 20, loss: 1.512833, error rate: 0.656589147287 \n",
      "epoch 21, loss: 1.476386, error rate: 0.654263565891 \n",
      "epoch 22, loss: 1.441486, error rate: 0.647286821705 \n",
      "epoch 23, loss: 1.408041, error rate: 0.640310077519 \n",
      "epoch 24, loss: 1.375968, error rate: 0.639534883721 \n",
      "epoch 25, loss: 1.345188, error rate: 0.638759689922 \n",
      "epoch 26, loss: 1.315632, error rate: 0.636434108527 \n",
      "epoch 27, loss: 1.287233, error rate: 0.632558139535 \n",
      "epoch 28, loss: 1.259930, error rate: 0.627131782946 \n",
      "epoch 29, loss: 1.233668, error rate: 0.627131782946 \n",
      "epoch 30, loss: 1.208393, error rate: 0.623255813953 \n",
      "epoch 31, loss: 1.184056, error rate: 0.619379844961 \n",
      "epoch 32, loss: 1.160611, error rate: 0.618604651163 \n",
      "epoch 33, loss: 1.138015, error rate: 0.614728682171 \n",
      "epoch 34, loss: 1.116227, error rate: 0.613953488372 \n",
      "epoch 35, loss: 1.095209, error rate: 0.611627906977 \n",
      "epoch 36, loss: 1.074926, error rate: 0.609302325581 \n",
      "epoch 37, loss: 1.055344, error rate: 0.605426356589 \n",
      "epoch 38, loss: 1.036430, error rate: 0.603875968992 \n",
      "epoch 39, loss: 1.018156, error rate: 0.601550387597 \n",
      "epoch 40, loss: 1.000492, error rate: 0.6 \n",
      "epoch 41, loss: 0.983412, error rate: 0.598449612403 \n",
      "epoch 42, loss: 0.966891, error rate: 0.596124031008 \n",
      "epoch 43, loss: 0.950904, error rate: 0.590697674419 \n",
      "epoch 44, loss: 0.935429, error rate: 0.587596899225 \n",
      "epoch 45, loss: 0.920444, error rate: 0.582945736434 \n",
      "epoch 46, loss: 0.905928, error rate: 0.576744186047 \n",
      "epoch 47, loss: 0.891863, error rate: 0.572868217054 \n",
      "epoch 48, loss: 0.878229, error rate: 0.570542635659 \n",
      "epoch 49, loss: 0.865009, error rate: 0.566666666667 \n",
      "epoch 50, loss: 0.852187, error rate: 0.56511627907 \n",
      "epoch 51, loss: 0.839746, error rate: 0.563565891473 \n",
      "epoch 52, loss: 0.827672, error rate: 0.557364341085 \n",
      "epoch 53, loss: 0.815950, error rate: 0.553488372093 \n",
      "epoch 54, loss: 0.804566, error rate: 0.550387596899 \n",
      "epoch 55, loss: 0.793507, error rate: 0.546511627907 \n",
      "epoch 56, loss: 0.782761, error rate: 0.545736434109 \n",
      "epoch 57, loss: 0.772316, error rate: 0.544186046512 \n",
      "epoch 58, loss: 0.762160, error rate: 0.541860465116 \n",
      "epoch 59, loss: 0.752283, error rate: 0.540310077519 \n",
      "epoch 60, loss: 0.742675, error rate: 0.541860465116 \n",
      "epoch 61, loss: 0.733325, error rate: 0.537984496124 \n",
      "epoch 62, loss: 0.724224, error rate: 0.535658914729 \n",
      "epoch 63, loss: 0.715362, error rate: 0.53488372093 \n",
      "epoch 64, loss: 0.706733, error rate: 0.531007751938 \n",
      "epoch 65, loss: 0.698326, error rate: 0.528682170543 \n",
      "epoch 66, loss: 0.690135, error rate: 0.52480620155 \n",
      "epoch 67, loss: 0.682151, error rate: 0.524031007752 \n",
      "epoch 68, loss: 0.674368, error rate: 0.520930232558 \n",
      "epoch 69, loss: 0.666778, error rate: 0.513953488372 \n",
      "epoch 70, loss: 0.659375, error rate: 0.513178294574 \n",
      "epoch 71, loss: 0.652152, error rate: 0.509302325581 \n",
      "epoch 72, loss: 0.645104, error rate: 0.506201550388 \n",
      "epoch 73, loss: 0.638224, error rate: 0.503100775194 \n",
      "epoch 74, loss: 0.631507, error rate: 0.497674418605 \n",
      "epoch 75, loss: 0.624948, error rate: 0.496124031008 \n",
      "epoch 76, loss: 0.618541, error rate: 0.493798449612 \n",
      "epoch 77, loss: 0.612281, error rate: 0.492248062016 \n",
      "epoch 78, loss: 0.606164, error rate: 0.490697674419 \n",
      "epoch 79, loss: 0.600184, error rate: 0.490697674419 \n",
      "epoch 80, loss: 0.594339, error rate: 0.490697674419 \n",
      "epoch 81, loss: 0.588623, error rate: 0.488372093023 \n",
      "epoch 82, loss: 0.583032, error rate: 0.486046511628 \n",
      "epoch 83, loss: 0.577562, error rate: 0.484496124031 \n",
      "epoch 84, loss: 0.572210, error rate: 0.482170542636 \n",
      "epoch 85, loss: 0.566972, error rate: 0.47984496124 \n",
      "epoch 86, loss: 0.561845, error rate: 0.47984496124 \n",
      "epoch 87, loss: 0.556825, error rate: 0.476744186047 \n",
      "epoch 88, loss: 0.551909, error rate: 0.474418604651 \n",
      "epoch 89, loss: 0.547093, error rate: 0.471317829457 \n",
      "epoch 90, loss: 0.542376, error rate: 0.468217054264 \n",
      "epoch 91, loss: 0.537754, error rate: 0.467441860465 \n",
      "epoch 92, loss: 0.533223, error rate: 0.467441860465 \n",
      "epoch 93, loss: 0.528782, error rate: 0.465891472868 \n",
      "epoch 94, loss: 0.524429, error rate: 0.461240310078 \n",
      "epoch 95, loss: 0.520159, error rate: 0.458139534884 \n",
      "epoch 96, loss: 0.515972, error rate: 0.457364341085 \n",
      "epoch 97, loss: 0.511864, error rate: 0.457364341085 \n",
      "epoch 98, loss: 0.507834, error rate: 0.456589147287 \n",
      "epoch 99, loss: 0.503880, error rate: 0.45503875969 \n",
      "epoch 100, loss: 0.499998, error rate: 0.45503875969 \n",
      "epoch 101, loss: 0.496188, error rate: 0.45503875969 \n",
      "epoch 102, loss: 0.492447, error rate: 0.451937984496 \n",
      "epoch 103, loss: 0.488773, error rate: 0.451937984496 \n",
      "epoch 104, loss: 0.485166, error rate: 0.451162790698 \n",
      "epoch 105, loss: 0.481622, error rate: 0.450387596899 \n",
      "epoch 106, loss: 0.478140, error rate: 0.450387596899 \n",
      "epoch 107, loss: 0.474719, error rate: 0.448837209302 \n",
      "epoch 108, loss: 0.471356, error rate: 0.446511627907 \n",
      "epoch 109, loss: 0.468051, error rate: 0.445736434109 \n",
      "epoch 110, loss: 0.464802, error rate: 0.44496124031 \n",
      "epoch 111, loss: 0.461608, error rate: 0.44496124031 \n",
      "epoch 112, loss: 0.458467, error rate: 0.44496124031 \n",
      "epoch 113, loss: 0.455377, error rate: 0.444186046512 \n",
      "epoch 114, loss: 0.452338, error rate: 0.442635658915 \n",
      "epoch 115, loss: 0.449348, error rate: 0.442635658915 \n",
      "epoch 116, loss: 0.446406, error rate: 0.441085271318 \n",
      "epoch 117, loss: 0.443511, error rate: 0.440310077519 \n",
      "epoch 118, loss: 0.440662, error rate: 0.439534883721 \n",
      "epoch 119, loss: 0.437857, error rate: 0.438759689922 \n",
      "epoch 120, loss: 0.435096, error rate: 0.437984496124 \n",
      "epoch 121, loss: 0.432377, error rate: 0.435658914729 \n",
      "epoch 122, loss: 0.429700, error rate: 0.433333333333 \n",
      "epoch 123, loss: 0.427063, error rate: 0.432558139535 \n",
      "epoch 124, loss: 0.424466, error rate: 0.431007751938 \n",
      "epoch 125, loss: 0.421907, error rate: 0.426356589147 \n",
      "epoch 126, loss: 0.419386, error rate: 0.427906976744 \n",
      "epoch 127, loss: 0.416902, error rate: 0.42480620155 \n",
      "epoch 128, loss: 0.414453, error rate: 0.423255813953 \n",
      "epoch 129, loss: 0.412040, error rate: 0.422480620155 \n",
      "epoch 130, loss: 0.409662, error rate: 0.421705426357 \n",
      "epoch 131, loss: 0.407317, error rate: 0.419379844961 \n",
      "epoch 132, loss: 0.405004, error rate: 0.418604651163 \n",
      "epoch 133, loss: 0.402724, error rate: 0.417054263566 \n",
      "epoch 134, loss: 0.400476, error rate: 0.416279069767 \n",
      "epoch 135, loss: 0.398258, error rate: 0.416279069767 \n",
      "epoch 136, loss: 0.396070, error rate: 0.413953488372 \n",
      "epoch 137, loss: 0.393912, error rate: 0.41007751938 \n",
      "epoch 138, loss: 0.391782, error rate: 0.408527131783 \n",
      "epoch 139, loss: 0.389681, error rate: 0.407751937984 \n",
      "epoch 140, loss: 0.387607, error rate: 0.406201550388 \n",
      "epoch 141, loss: 0.385560, error rate: 0.406201550388 \n",
      "epoch 142, loss: 0.383539, error rate: 0.403875968992 \n",
      "epoch 143, loss: 0.381545, error rate: 0.402325581395 \n",
      "epoch 144, loss: 0.379576, error rate: 0.401550387597 \n",
      "epoch 145, loss: 0.377631, error rate: 0.400775193798 \n",
      "epoch 146, loss: 0.375711, error rate: 0.398449612403 \n",
      "epoch 147, loss: 0.373815, error rate: 0.398449612403 \n",
      "epoch 148, loss: 0.371942, error rate: 0.398449612403 \n",
      "epoch 149, loss: 0.370092, error rate: 0.396899224806 \n",
      "epoch 150, loss: 0.368265, error rate: 0.395348837209 \n",
      "epoch 151, loss: 0.366459, error rate: 0.390697674419 \n",
      "epoch 152, loss: 0.364675, error rate: 0.38992248062 \n",
      "epoch 153, loss: 0.362913, error rate: 0.388372093023 \n",
      "epoch 154, loss: 0.361170, error rate: 0.389147286822 \n",
      "epoch 155, loss: 0.359449, error rate: 0.388372093023 \n",
      "epoch 156, loss: 0.357747, error rate: 0.389147286822 \n",
      "epoch 157, loss: 0.356064, error rate: 0.388372093023 \n",
      "epoch 158, loss: 0.354401, error rate: 0.386046511628 \n",
      "epoch 159, loss: 0.352757, error rate: 0.386046511628 \n",
      "epoch 160, loss: 0.351131, error rate: 0.386046511628 \n",
      "epoch 161, loss: 0.349523, error rate: 0.385271317829 \n",
      "epoch 162, loss: 0.347934, error rate: 0.383720930233 \n",
      "epoch 163, loss: 0.346361, error rate: 0.382170542636 \n",
      "epoch 164, loss: 0.344806, error rate: 0.380620155039 \n",
      "epoch 165, loss: 0.343267, error rate: 0.378294573643 \n",
      "epoch 166, loss: 0.341745, error rate: 0.378294573643 \n",
      "epoch 167, loss: 0.340239, error rate: 0.377519379845 \n",
      "epoch 168, loss: 0.338750, error rate: 0.37519379845 \n",
      "epoch 169, loss: 0.337275, error rate: 0.373643410853 \n",
      "epoch 170, loss: 0.335817, error rate: 0.373643410853 \n",
      "epoch 171, loss: 0.334373, error rate: 0.373643410853 \n",
      "epoch 172, loss: 0.332944, error rate: 0.373643410853 \n",
      "epoch 173, loss: 0.331530, error rate: 0.372093023256 \n",
      "epoch 174, loss: 0.330130, error rate: 0.372093023256 \n",
      "epoch 175, loss: 0.328744, error rate: 0.368992248062 \n",
      "epoch 176, loss: 0.327372, error rate: 0.36976744186 \n",
      "epoch 177, loss: 0.326014, error rate: 0.368217054264 \n",
      "epoch 178, loss: 0.324669, error rate: 0.368217054264 \n",
      "epoch 179, loss: 0.323337, error rate: 0.365891472868 \n",
      "epoch 180, loss: 0.322018, error rate: 0.36511627907 \n",
      "epoch 181, loss: 0.320712, error rate: 0.364341085271 \n",
      "epoch 182, loss: 0.319418, error rate: 0.363565891473 \n",
      "epoch 183, loss: 0.318137, error rate: 0.365891472868 \n",
      "epoch 184, loss: 0.316868, error rate: 0.365891472868 \n",
      "epoch 185, loss: 0.315610, error rate: 0.364341085271 \n",
      "epoch 186, loss: 0.314365, error rate: 0.363565891473 \n",
      "epoch 187, loss: 0.313131, error rate: 0.363565891473 \n",
      "epoch 188, loss: 0.311908, error rate: 0.362790697674 \n",
      "epoch 189, loss: 0.310696, error rate: 0.362015503876 \n",
      "epoch 190, loss: 0.309496, error rate: 0.362015503876 \n",
      "epoch 191, loss: 0.308306, error rate: 0.361240310078 \n",
      "epoch 192, loss: 0.307127, error rate: 0.360465116279 \n",
      "epoch 193, loss: 0.305958, error rate: 0.358139534884 \n",
      "epoch 194, loss: 0.304800, error rate: 0.355813953488 \n",
      "epoch 195, loss: 0.303652, error rate: 0.353488372093 \n",
      "epoch 196, loss: 0.302514, error rate: 0.351162790698 \n",
      "epoch 197, loss: 0.301386, error rate: 0.351162790698 \n",
      "epoch 198, loss: 0.300267, error rate: 0.351937984496 \n",
      "epoch 199, loss: 0.299158, error rate: 0.351162790698 \n",
      "epoch 200, loss: 0.298059, error rate: 0.350387596899 \n",
      "epoch 201, loss: 0.296969, error rate: 0.348837209302 \n",
      "epoch 202, loss: 0.295887, error rate: 0.348062015504 \n",
      "epoch 203, loss: 0.294815, error rate: 0.346511627907 \n",
      "epoch 204, loss: 0.293752, error rate: 0.34496124031 \n",
      "epoch 205, loss: 0.292698, error rate: 0.343410852713 \n",
      "epoch 206, loss: 0.291652, error rate: 0.343410852713 \n",
      "epoch 207, loss: 0.290615, error rate: 0.342635658915 \n",
      "epoch 208, loss: 0.289586, error rate: 0.341860465116 \n",
      "epoch 209, loss: 0.288565, error rate: 0.341085271318 \n",
      "epoch 210, loss: 0.287553, error rate: 0.340310077519 \n",
      "epoch 211, loss: 0.286549, error rate: 0.340310077519 \n",
      "epoch 212, loss: 0.285552, error rate: 0.340310077519 \n",
      "epoch 213, loss: 0.284564, error rate: 0.337984496124 \n",
      "epoch 214, loss: 0.283583, error rate: 0.337984496124 \n",
      "epoch 215, loss: 0.282609, error rate: 0.337984496124 \n",
      "epoch 216, loss: 0.281643, error rate: 0.337984496124 \n",
      "epoch 217, loss: 0.280685, error rate: 0.337984496124 \n",
      "epoch 218, loss: 0.279734, error rate: 0.337209302326 \n",
      "epoch 219, loss: 0.278790, error rate: 0.337209302326 \n",
      "epoch 220, loss: 0.277853, error rate: 0.337209302326 \n",
      "epoch 221, loss: 0.276924, error rate: 0.335658914729 \n",
      "epoch 222, loss: 0.276001, error rate: 0.334108527132 \n",
      "epoch 223, loss: 0.275085, error rate: 0.332558139535 \n",
      "epoch 224, loss: 0.274175, error rate: 0.331782945736 \n",
      "epoch 225, loss: 0.273273, error rate: 0.331782945736 \n",
      "epoch 226, loss: 0.272377, error rate: 0.331007751938 \n",
      "epoch 227, loss: 0.271487, error rate: 0.331007751938 \n",
      "epoch 228, loss: 0.270604, error rate: 0.331007751938 \n",
      "epoch 229, loss: 0.269727, error rate: 0.33023255814 \n",
      "epoch 230, loss: 0.268857, error rate: 0.329457364341 \n",
      "epoch 231, loss: 0.267992, error rate: 0.328682170543 \n",
      "epoch 232, loss: 0.267134, error rate: 0.327906976744 \n",
      "epoch 233, loss: 0.266282, error rate: 0.327131782946 \n",
      "epoch 234, loss: 0.265436, error rate: 0.327131782946 \n",
      "epoch 235, loss: 0.264595, error rate: 0.327131782946 \n",
      "epoch 236, loss: 0.263761, error rate: 0.326356589147 \n",
      "epoch 237, loss: 0.262932, error rate: 0.326356589147 \n",
      "epoch 238, loss: 0.262109, error rate: 0.325581395349 \n",
      "epoch 239, loss: 0.261291, error rate: 0.32480620155 \n",
      "epoch 240, loss: 0.260479, error rate: 0.324031007752 \n",
      "epoch 241, loss: 0.259672, error rate: 0.324031007752 \n",
      "epoch 242, loss: 0.258871, error rate: 0.323255813953 \n",
      "epoch 243, loss: 0.258075, error rate: 0.323255813953 \n",
      "epoch 244, loss: 0.257285, error rate: 0.320930232558 \n",
      "epoch 245, loss: 0.256499, error rate: 0.319379844961 \n",
      "epoch 246, loss: 0.255719, error rate: 0.319379844961 \n",
      "epoch 247, loss: 0.254944, error rate: 0.319379844961 \n",
      "epoch 248, loss: 0.254174, error rate: 0.318604651163 \n",
      "epoch 249, loss: 0.253409, error rate: 0.317829457364 \n",
      "epoch 250, loss: 0.252649, error rate: 0.317829457364 \n",
      "epoch 251, loss: 0.251893, error rate: 0.316279069767 \n",
      "epoch 252, loss: 0.251143, error rate: 0.316279069767 \n",
      "epoch 253, loss: 0.250397, error rate: 0.314728682171 \n",
      "epoch 254, loss: 0.249656, error rate: 0.314728682171 \n",
      "epoch 255, loss: 0.248920, error rate: 0.313953488372 \n",
      "epoch 256, loss: 0.248188, error rate: 0.313953488372 \n",
      "epoch 257, loss: 0.247461, error rate: 0.313178294574 \n",
      "epoch 258, loss: 0.246739, error rate: 0.311627906977 \n",
      "epoch 259, loss: 0.246020, error rate: 0.310852713178 \n",
      "epoch 260, loss: 0.245307, error rate: 0.31007751938 \n",
      "epoch 261, loss: 0.244597, error rate: 0.310852713178 \n",
      "epoch 262, loss: 0.243892, error rate: 0.31007751938 \n",
      "epoch 263, loss: 0.243192, error rate: 0.31007751938 \n",
      "epoch 264, loss: 0.242495, error rate: 0.309302325581 \n",
      "epoch 265, loss: 0.241803, error rate: 0.306976744186 \n",
      "epoch 266, loss: 0.241115, error rate: 0.306201550388 \n",
      "epoch 267, loss: 0.240431, error rate: 0.305426356589 \n",
      "epoch 268, loss: 0.239751, error rate: 0.304651162791 \n",
      "epoch 269, loss: 0.239075, error rate: 0.303100775194 \n",
      "epoch 270, loss: 0.238403, error rate: 0.303100775194 \n",
      "epoch 271, loss: 0.237736, error rate: 0.303100775194 \n",
      "epoch 272, loss: 0.237072, error rate: 0.303100775194 \n",
      "epoch 273, loss: 0.236411, error rate: 0.303100775194 \n",
      "epoch 274, loss: 0.235755, error rate: 0.303100775194 \n",
      "epoch 275, loss: 0.235103, error rate: 0.303100775194 \n",
      "epoch 276, loss: 0.234454, error rate: 0.303100775194 \n",
      "epoch 277, loss: 0.233809, error rate: 0.303100775194 \n",
      "epoch 278, loss: 0.233168, error rate: 0.303100775194 \n",
      "epoch 279, loss: 0.232530, error rate: 0.302325581395 \n",
      "epoch 280, loss: 0.231896, error rate: 0.301550387597 \n",
      "epoch 281, loss: 0.231266, error rate: 0.302325581395 \n",
      "epoch 282, loss: 0.230639, error rate: 0.301550387597 \n",
      "epoch 283, loss: 0.230016, error rate: 0.3 \n",
      "epoch 284, loss: 0.229396, error rate: 0.299224806202 \n",
      "epoch 285, loss: 0.228779, error rate: 0.299224806202 \n",
      "epoch 286, loss: 0.228166, error rate: 0.298449612403 \n",
      "epoch 287, loss: 0.227557, error rate: 0.297674418605 \n",
      "epoch 288, loss: 0.226951, error rate: 0.298449612403 \n",
      "epoch 289, loss: 0.226348, error rate: 0.298449612403 \n",
      "epoch 290, loss: 0.225748, error rate: 0.298449612403 \n",
      "epoch 291, loss: 0.225152, error rate: 0.298449612403 \n",
      "epoch 292, loss: 0.224559, error rate: 0.298449612403 \n",
      "epoch 293, loss: 0.223970, error rate: 0.298449612403 \n",
      "epoch 294, loss: 0.223383, error rate: 0.297674418605 \n",
      "epoch 295, loss: 0.222800, error rate: 0.297674418605 \n",
      "epoch 296, loss: 0.222219, error rate: 0.297674418605 \n",
      "epoch 297, loss: 0.221642, error rate: 0.297674418605 \n",
      "epoch 298, loss: 0.221068, error rate: 0.297674418605 \n",
      "epoch 299, loss: 0.220497, error rate: 0.296899224806 \n"
     ]
    }
   ],
   "source": [
    "activate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
